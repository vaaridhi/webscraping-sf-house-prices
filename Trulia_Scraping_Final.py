# -*- coding: utf-8 -*-
"""Trulia_Page_Download.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a-Dm7IOLs_Xaw-qpMlNV0Mk7bV42leVH
"""

import os
import time
from bs4 import BeautifulSoup
import requests
from tqdm import tqdm
import pandas as pd
import json

def trulia_url_fetch(): 
    #List of URLs to traverse to pull all the home URLs - each corresponds a different price category
    listhome_url = [
    'https://www.trulia.com/sold/San_Francisco,CA/0-800000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/800000-1100000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/1100000-1250000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/1250000-1500000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/1500000-1900000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/1900000-3000000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/',
    'https://www.trulia.com/sold/San_Francisco,CA/3000000-20000000_price/APARTMENT,CONDO,COOP,MULTI-FAMILY,SINGLE-FAMILY_HOME,TOWNHOUSE_type/'
    ]
    #Number of pages in each of these URLs
    listhome_size = [
          21,
          21,
          14,
          19,
          21,
          19,
          11
      ]

    headers = {
              'User-Agent': 'Mozilla 5.0'
    }
    #Looping over the 7 URLs/Price Categories
    for j in tqdm(range(0,7)):
      list_content = []
      url_base = listhome_url[j]
      #Looping over each of the listings pages in each of the pricing category
      for i in tqdm(range(1,listhome_size[j])):
          print(f"downloading trulia page no. {j}, {i}")
          yp_url = url_base + f'{i}_p/'
          
          page = requests.get(yp_url, headers=headers)
          # Create a beautifulsoup object
          content_soup = BeautifulSoup(page.content, 'html.parser') 

          list_homes = content_soup.findAll("div", attrs={'data-testid':"home-card-sale"})

          #Extracting the URLs of each of the homes on a page
          for x, item in enumerate(list_homes):
              url_item = item.find("a", attrs={'class':"Anchor__StyledAnchor-sc-5lya7g-1 jYyvOn"})["href"]
              url_home = "https://www.trulia.com" + url_item
              list_content = list_content + [url_home]

          time.sleep(10)

      #Writing URLs of a price category to a list
      file_path = f'/content/drive/MyDrive/Projects/Trulia Scraping/urls/url{j}'
      with open(file_path, 'w') as fp:
        for item in list_content:
            # write each item on a new line
            fp.write("%s\n" % item)

#Feature Extraction
def feature_extract_soup(soup_each_house, url_1):
    house_details_dict = {}
    house_details_dict['URL'] = url_1
    
    #Feature Extraction - higher level details in the header
    try:
        house_details_dict['media_count'] = soup_each_house.find('span', attrs={'data-testid' : "media-count"}).text
    except:
        house_details_dict['media_count'] = "NA"
    try:
        house_details_dict['Price'] = soup_each_house.find('div', attrs={'data-testid' : "home-price-details-medium"}).find('h3').text
    except:
        house_details_dict['Price'] = "NA"
    try:
        add_temp = soup_each_house.find('div', attrs={'data-testid':"home-details-summary-address"}).findAll('span')
        house_details_dict['Address'] = add_temp[0].text + ", " +add_temp[1].text
    except:
        house_details_dict['Address'] = "NA"
    try:
        house_details_dict['Last_Sold'] = soup_each_house.find('div', attrs={'class':"Text__TextBase-sc-1cait9d-0-div Text__TextContainerBase-sc-1cait9d-1 csrRqu gUnlde"}).text
    except:
        house_details_dict['Last_Sold'] = "NA"
    try:
        house_details_dict['Beds'] = soup_each_house.find('div', attrs={'data-testid':"home-summary-size-bedrooms"}).text
    except:
        house_details_dict['Beds'] = "NA"
    try:
        house_details_dict['Baths'] = soup_each_house.find('div', attrs={'data-testid':"home-summary-size-bathrooms"}).text
    except:
        house_details_dict['Baths'] = "NA"
    try:
        house_details_dict['Floor_Space'] = soup_each_house.find('div', attrs={'data-testid':"home-summary-size-floorspace"}).text
    except:
        house_details_dict['Floor_Space'] = "NA"
    
    #Updating dictionaries with information from the sections like interior details
    try:        
        table_list = soup_each_house.find('div', attrs={'data-testid':"features-container"}).findAll('table', attrs={'class':'Table-latbb5-3 hMYIhS'})
        for table in table_list:
            heading = table.find('h3').text
            house_details_dict[heading] = {}
            for row in table.find('tbody').findAll('tr'):
                subheading = row.find('div').text
                row_text = []
                for i in row.findAll('span', attrs = {'class':"Feature__FeatureListItem-sc-w1mxt5-0 gmLKqq"}):
                    if ':' in i.text:
                        dict_temp = {}
                        dict_temp[i.text.split(':')[0].strip()] = i.text.split(':')[1].strip()
                        row_text.append(dict_temp)
                    else:
                        row_text.append(i.text)
                house_details_dict[heading][subheading] = row_text
            if heading == "Miscellaneous":
                break
    except Exception as e:
        print(e)

    #Using position stack API to retrieve latitude and longitude information
    base_url = "http://api.positionstack.com/v1/forward?access_key=e2abd687d99762d5093b6721625a1bed&query="

    for i in tqdm(range(0,house_data_df.shape[0]+1)):
        try:
            add1 = house_details_dict['Address']
            url = base_url + quote(add1)
            respage = requests.get(url).json()
            house_details_dict['latitude'] = respage['data'][0]['latitude']
            house_details_dict['longitude'] = respage['data'][0]['longitude']
        except:
            house_details_dict['latitude'] = 'NA'
            house_details_dict['longitude'] = 'NA'
      
    return house_details_dict

#Downloading the HTML files and Extracting features
def home_download(n):
    headers = {
      'User-Agent': 'Mozilla 5.0'
    }
    house_dict_list = []
    
    filename1 = f'/content/drive/MyDrive/Projects/Trulia Scraping/urls/url{n}'

    df1 = pd.read_csv(filename1, sep='\s+', names=['url'])
    url_list = df1['url']
    for idx, url in tqdm(enumerate(url_list)):
        page = requests.get(url, headers=headers)
        # Create a beautifulsoup object
        content_soup = BeautifulSoup(page.content, 'html.parser')

        house_dict = feature_extract_soup(content_soup, url)
        house_dict_list.append(house_dict)

        file_name = f"/content/drive/MyDrive/Projects/Trulia Scraping/house_html/trulia_sf_home_page_{n}_{idx}.htm"

        with open(file_name, 'rb', encoding="utf-8") as file:
            file.write(str(content_soup))

        time.sleep(5)
    
    return house_dict_list

def json_to_mongodb():
    client = pymongo.MongoClient("mongodb+srv://@cluster0..mongodb.net/?retryWrites=true&w=majority")
    ddr_db = client["DDR_Proj_Trulia"]
    trulia_collection = ddr_db["Trulia_house_details"]
    house_details_list = []
    folder_path = '/content/drive/MyDrive/Projects/Trulia Scraping/json output'
    for filename in tqdm(os.listdir(folder_path)):
            jsonfile = folder_path + "/" + filename
            with open(jsonfile, 'r') as jsonoutput:
                house_details_dict = json.load(jsonoutput)
                house_details_list.append(house_details_dict)
    url_list = []
    for i in house_details_list:
      for j in i:
        url_list.append(j['URL'])
    
    for item in house_details_list:
        trulia_collection.insert_one(item)

def json_features_extraction():
  # Create a list to store the data
  rows = []
  id_ = 0
  folder_path = '/content/drive/MyDrive/Projects/Trulia Scraping/json output'
  for filename in tqdm(os.listdir(folder_path)):
          jsonfile = folder_path + "/" + filename
          with open(jsonfile, 'r') as f:
          data = json.load(f)
          # Iterate over each property information
          for property_info in data:
              # Extract the desired information from the property_info dictionary
              id_ = id_+1
              price = property_info.get('Price', '')
              address = property_info.get('Address', '')

              # The year of Last_Sold
              last_sold = property_info.get('Last_Sold', '')
              match = re.search('\d{4}', last_sold)
              if match:
                  year_sold = match.group()
              else:
                  year_sold = 'None'

              beds = property_info.get('Beds', '')
              bath = property_info.get('Baths', '')
              floor_space = property_info.get('Floor_Space', '')
              year_built = property_info.get('Property Information', {}).get('Year Built', [{}])[0].get('Year Built', '')

              # num of rooms
              num_of_rooms_el = property_info.get('Interior Features', {}).get('Interior Details', [{}])[0]
              if 'Number of Rooms' in num_of_rooms_el:
                  num_of_rooms = property_info.get('Interior Features', {}).get('Interior Details', [{}])[0].get('Number of Rooms')
              else:
                  num_of_rooms = 'None'

              # check for internet, gas, Refrigerator, washer, dryer, dishwasher, Microwave
              utilities_el = property_info.get('Interior Features', {}).get('Appliances & Utilities', [{}])
              utilities = str(utilities_el).lower()
              if 'internet' in utilities:
                  internet = 1
              else:
                  internet = 0
              if 'gas' in utilities:
                  gas = 1
              else:
                  gas = 0
              if 'refrigerator' in utilities:
                  refrigerator = 1
              else:
                  refrigerator = 0
              if 'washer' in utilities:
                  washer = 1
              else:
                  washer = 0
              if 'dryer' in utilities:
                  dryer = 1
              else:
                  dryer = 0
              if 'microwave' in utilities:
                  microwave = 1
              else:
                  microwave = 0

              # views
              views_el = property_info.get('Interior Features', {}).get('View', [{}])[0]
              views = str(views_el)

              # check for parkings, Carport, Garage, Open_Parking
              parkings = property_info.get('Exterior Features', {}).get('Parking & Garage', [{}])
              if 'Has a Carport' in str(parkings):
                  parking_carport = 1
              else:
                  parking_carport = 0
              if 'Has a Garage' in str(parkings):
                  parking_garage = 1
              else:
                  parking_garage = 0
              if 'Has Open Parking' in str(parkings):
                  parking_open = 1
              else:
                  parking_open = 0

              property_type = property_info.get('Property Information', {}).get('Property Type / Style', [{}])[0].get('Property Type')
              condition = property_info.get('Property Information', {}).get('Property Information', [{}])[0].get('Condition')
              latitude = property_info.get('latitude', '')
              longitude = property_info.get('longitude', '')
              # Add the extracted information as a tuple to the list of rows
              rows.append((id_, price, address, year_sold, beds, bath, floor_space, year_built, num_of_rooms, internet, gas, refrigerator, washer, dryer, microwave, views, parking_carport, parking_garage, parking_open, property_type, condition, latitude, longitude))

              df_features = pd.DataFrame(rows, columns=['ID', 'Price', 'Address', 'Years Sold', 'Bed', 'Bath', 'Floor Space', 'Year Built', 'Num of Rooms', 'Internet', 'Gas', 'Refrigerator', 'Washer', 'Dryer', 'Microwave', 'Views', 'Parking-Carport', 'Parking-Garage', 'Parking-Open', 'Property Type', 'Condition','latitude','longitude'], index=range(1, len(rows) + 1))

              return(df_features)

def data_processing(df_features):
    house_data_df = df_features
    house_data_df['Price'] = house_data_df['Price'].apply(lambda x: np.nan if '/mo' in str(x) else x)
    house_data_df['Price'] = house_data_df['Price'].str.replace(',', '').str.replace('$', '').fillna(0).astype(int)
    house_data_df['Zip_Code'] = house_data_df['Address'].str.split(' ').str[-1]
    house_data_df['Bed'] = house_data_df['Bed'].apply(lambda x: np.nan if 'Studio' in str(x) else x)
    house_data_df['Bed'] = house_data_df['Bed'].str.split(' ').str[0].fillna(0).astype(int)
    house_data_df['Bath'] = house_data_df['Bath'].str.split(' ').str[0].fillna(0).astype(float)
    house_data_df['Floor Space'] = house_data_df['Floor Space'].str.split(' ').str[0].str.replace(',','').fillna(0).astype(int)

    #there are no values that are 0 for num of rooms to imputing none and nan as 0
    house_data_df['Num of Rooms'] = house_data_df['Num of Rooms'].replace('None', np.nan).fillna(0).astype(int)

    house_data_df.to_csv('/content/drive/MyDrive/Projects/Trulia Scraping/processed_data_transformed.csv')

if __name__ == "__main__":
    #Downloading individual pages of the listing and extracting features to json format
    for i in range(0,7):
      house_dict_list = home_download(i)
      #Writing the output dictionaries to JSON format files
      with open(f'/content/drive/MyDrive/Projects/Trulia Scraping/json output/{i}_output.json', 'w') as fout:
          json.dump(house_dict_list , fout)
    
    #Pushing the data to MongoDB
    json_to_mongodb()

    #Extract features from JSON structure
    df_features = json_features_extraction()

    #Data processing - exporting to csv - final dataset ready for analysis
    data_processing(df_features)